<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>text mining on StatLab Articles</title>
    <link>/tags/text-mining/</link>
    <description>Recent content in text mining on StatLab Articles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/text-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reading PDF files into R for text mining</title>
      <link>/2019/05/14/reading-pdf-files-into-r-for-text-mining/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/14/reading-pdf-files-into-r-for-text-mining/</guid>
      <description>Let’s say we’re interested in text mining the opinions of The Supreme Court of the United States from the 2014 term. The opinions are published as PDF files at the following web page http://www.supremecourt.gov/opinions/slipopinion/14. We would probably want to look at all 76 opinions, but for the purposes of this introductory tutorial we’ll just look at the last three of the term: (1) Glossip v. Gross, (2) State Legislature v.</description>
    </item>
    
    <item>
      <title>An Introduction to Analyzing Twitter Data with R</title>
      <link>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</guid>
      <description>In this article, I will walk you through why a researcher or professional might find data from Twitter useful, explain how to collect the relevant tweets and information from Twitter in R, and then finish by demonstrating a few useful analyses (along with accompanying cleaning) you might perform on your Twitter data.
Part One: Why Twitter Data?To begin, we should ask: why would someone be interested in using data from Twitter?</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 5</title>
      <link>/2019/01/31/analysis-of-ours-to-shape-comments-part-5/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/31/analysis-of-ours-to-shape-comments-part-5/</guid>
      <description>IntroductionIn the penultimate post of this series, we’ll use some unsupervised learning approaches to uncover comment clusters and latent themes among the comments to President Ryan’s Ours to Shape website.
library(quanteda) # main text packagelibrary(tidyverse) # for dplyr, stringr, piping, etc.library(RColorBrewer) # better colors in graphslibrary(scales) # better breaks and labels in graphslibrary(stm) # structural topic modelsLet’s start with the document-feature matrix we created from the de-duplicated comments.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 4</title>
      <link>/2018/12/19/analysis-of-ours-to-shape-comments-part-4/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/19/analysis-of-ours-to-shape-comments-part-4/</guid>
      <description>IntroductionIn the fourth installment of this series (we’re almost done, I promise), we’ll look at the sentiment – aka positive/negative tone, polarity, affect – of the comments to President Ryan’s Ours to Shape website.
We don’t have a pre-labeled set of comments, with negative or positive sentiment already identified, so we can’t use a supervised classification method (and I’m not committed enough to hand code a sample of comments).</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 3</title>
      <link>/2018/12/18/analysis-of-ours-to-shape-comments-part-3/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/18/analysis-of-ours-to-shape-comments-part-3/</guid>
      <description>IntroductionTo recap, we’re exploring the comments submitted to President Ryan’s Ours to Shape website (as of December 7, 2018).
In the first post we looked at the distribution of comments across Ryan’s three categories – community, discovery, and service – and across the contributors’ primary connection to the university. We extracted features like length and readability of the comments, and compared these across groups. And we explored the context in which key words of interest (to me) were used.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 2</title>
      <link>/2018/12/14/analysis-of-ours-to-shape-comments-part-2/</link>
      <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/14/analysis-of-ours-to-shape-comments-part-2/</guid>
      <description>IntroductionIn the last post, we began exploring the Ours to Shape comments – the distribution across categories and contributors, the length and readability of the comments, and a few key words in context. While I did more exploration of the data than reported, the first post gives a taste of the kind of dive into the data that usefully proceeds analysis.
In this post, we’ll start digging into word frequencies, relative frequencies by groups, and distinguishing words.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 1</title>
      <link>/2018/12/13/analysis-of-ours-to-shape-comments-part-1/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/13/analysis-of-ours-to-shape-comments-part-1/</guid>
      <description>IntroductionAs part of a series of workshops on quantitative analysis of text this fall, I started examining the comments submitted to President Ryan’s Ours to Shape website. The site invites people to share their ideas and insights for UVA going forward, particularly in the domains of service, discovery, and community. The website was only one venue for providing suggestions and voicing possibilities – President Ryan has hosted community discussions as well – but the website afforded an opportunity for individuals to chime in multiple times and at their convenience, so in theory should represent an especially inclusive collection.</description>
    </item>
    
    <item>
      <title>A Beginner’s Guide to Text Analysis with quanteda</title>
      <link>/2018/11/27/a-beginner-s-guide-to-text-analysis-with-quanteda/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/27/a-beginner-s-guide-to-text-analysis-with-quanteda/</guid>
      <description>A lot of introductory tutorials to quanteda assume that the reader has some base of knowledge about the program’s functionality or how it might be used. Other tutorials assume that the user is an expert in R and on what goes on under the hood when you’re coding. This introductory guide will assume none of that. Instead, I’m presuming a very basic understanding of R (like how to assign variables) and that you’ve just heard of quanteda for the first time today.</description>
    </item>
    
  </channel>
</rss>